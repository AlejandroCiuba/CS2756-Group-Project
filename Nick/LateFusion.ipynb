{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a4b319e-7bfa-434f-a472-c0dad85bbe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, DatasetDict, Features, Dataset, Value, ClassLabel, Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7bd00c5-d51f-4007-8f72-7381d158e3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4b477c94bb40c1b60d1b0f481e2751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/13311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28cf324a4f734b0e8e2766dccc3e1c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1903 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "731d7d6540204ccea99adafb0e959c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/3806 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"imagefolder\", data_dir=\"/data_vault/hexai/ArtEmis-FinalSplits/\")\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(dataset[\"train\"].features[\"label\"].names)}\n",
    "id2label = {i: label for label, i in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56ecb83f-4ac2-4892-841c-25a1eba3ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "alpha_re = re.compile(r\"[^a-zA-Z-_0-9]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0fb4cbc-0303-45ce-aec8-82d0575e7480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'utf-8'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getdefaultencoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "156b52eb-30db-475b-b1a3-34c94e63ec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\"final-splits.csv\")\n",
    "\n",
    "def replace_unicode(x):\n",
    "    unicode = {uni: \"a\" for uni in ['ã¶', 'ã\\xad', 'ã©', 'ã¨', 'ã³', 'ã¼', 'â\\xa0']}\n",
    "    for uni in unicode:\n",
    "        x = x.replace(uni, unicode[uni])\n",
    "   \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45ae2f81-da22-4b2b-83b4-06364c8881c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[\"painting\"] = metadata[\"painting\"].apply(lambda x: replace_unicode(x))\n",
    "metadata[\"painting\"] = metadata[\"painting\"].apply(lambda x: alpha_re.sub(\"\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30d96ef2-06db-42c5-9f6a-854ec6984fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def add_utterance_data(dataset, metadata):\n",
    "    # Training dataset\n",
    "    train_utterances = []\n",
    "    for batch in tqdm(dataset[\"train\"]):\n",
    "        painting = batch[\"image\"].filename.split(\"/\")[-1].split(\".\")[0]\n",
    "        painting = alpha_re.sub(\"\", painting)     \n",
    "        meta_utt = metadata[metadata.painting.str.contains(painting)][\"utterance\"].values[0]\n",
    "        train_utterances.append(meta_utt)\n",
    "    \n",
    "    dataset[\"train\"] = dataset[\"train\"].add_column(\"utterance\", train_utterances)\n",
    "\n",
    "    # Validation dataset\n",
    "    valid_utterances = []\n",
    "    for batch in tqdm(dataset[\"validation\"]):\n",
    "        painting = batch[\"image\"].filename.split(\"/\")[-1].split(\".\")[0]\n",
    "        painting = alpha_re.sub(\"\", painting)     \n",
    "        meta_utt = metadata[metadata.painting.str.contains(painting)][\"utterance\"].values[0]\n",
    "        valid_utterances.append(meta_utt)\n",
    "    \n",
    "    dataset[\"validation\"] = dataset[\"validation\"].add_column(\"utterance\", valid_utterances)\n",
    "\n",
    "    # Testing dataset\n",
    "    test_utterances = []\n",
    "    for batch in tqdm(dataset[\"test\"]):\n",
    "        painting = batch[\"image\"].filename.split(\"/\")[-1].split(\".\")[0]\n",
    "        painting = alpha_re.sub(\"\", painting)     \n",
    "        meta_utt = metadata[metadata.painting.str.contains(painting)][\"utterance\"].values[0]\n",
    "        test_utterances.append(meta_utt)\n",
    "    \n",
    "    dataset[\"test\"] = dataset[\"test\"].add_column(\"utterance\", test_utterances)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d72adf9b-19f0-497b-aebf-4a10c4457ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 13311/13311 [03:12<00:00, 69.15it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 1903/1903 [00:27<00:00, 69.57it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 3806/3806 [00:54<00:00, 69.61it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = add_utterance_data(dataset, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f08e6e9-7e21-4765-b96e-753930a9cf1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label', 'utterance'],\n",
       "    num_rows: 3806\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2ebf23-109d-4f2f-b9de-8b0b0c07e154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6257cd-189c-4698-96b4-d32441755b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e905f296-1d4c-449f-81ef-ac79787f7771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoImageProcessor, AutoModelForImageClassification, AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "\n",
    "vision_checkpoint = \"efficientnet_best/\"\n",
    "bert_checkpoint = \"bert_best/\"\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(vision_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\", do_lower_case=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])\n",
    "\n",
    "def image_transforms(examples):\n",
    "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    " \n",
    "    del examples[\"image\"]\n",
    "    return examples\n",
    "\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples['utterance'], padding='max_length', truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8cda71a-041f-45f4-b0ff-62631f900c7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_dataset = dataset.map(tokenize, batched=True)\n",
    "text_dataset.set_format(type='torch', columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d56dd916-52ce-4a66-adb6-24caed9dd24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset = dataset.with_transform(image_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "801fa8e8-db4e-4280-a689-b0f2c9d0bd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = AutoModelForImageClassification.from_pretrained(\"vit_best/\").cuda()\n",
    "text_model = AutoModelForSequenceClassification.from_pretrained(\"bert_best_v2/\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "58762493-c9b8-474e-83d2-00db665af3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class LateFusionModel(nn.Module):\n",
    "    def __init__(self, cv_model, text_model, weights = [0.5, 0.5]):\n",
    "        super(LateFusionModel, self).__init__()\n",
    "        self.cv_model = cv_model\n",
    "        self.text_model = text_model\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, image, text, attention_mask):\n",
    "        img_out = self.cv_model(image)\n",
    "        text_out = self.text_model(text, attention_mask)\n",
    "\n",
    "        return self.weights[0] * self.softmax(img_out.logits) + self.weights[1] * self.softmax(text_out.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "45b5c29f-0a3c-4dbe-997f-e361f8b60f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfm = LateFusionModel(image_model, text_model, weights=[0.75, 0.25]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8ed1dc5c-6e62-47f9-933f-c1d32aa6cdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = next(iter(image_dataset[\"test\"]))\n",
    "txt_data = next(iter(text_dataset[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f75ec25a-60d2-4f83-ae60-d97812dab16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels = img_data[\"pixel_values\"].cuda().unsqueeze(dim=0)\n",
    "input_ids = txt_data[\"input_ids\"].cuda().unsqueeze(dim=0)\n",
    "attn_mask = txt_data[\"attention_mask\"].cuda().unsqueeze(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b67b7ae4-e254-44d5-aff0-72a1d1c59a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, labels = [], []\n",
    "for img_batch, txt_batch in zip(image_dataset[\"test\"], text_dataset[\"test\"]):\n",
    "    pixels = img_batch[\"pixel_values\"].cuda().unsqueeze(dim=0)\n",
    "    input_ids = txt_data[\"input_ids\"].cuda().unsqueeze(dim=0)\n",
    "    attn_mask = txt_data[\"attention_mask\"].cuda().unsqueeze(dim=0)\n",
    "    \n",
    "    out = lfm(pixels, input_ids, attn_mask)\n",
    "    predictions = np.argmax(out.detach().cpu().numpy(), axis=-1)\n",
    "    preds.extend(predictions)\n",
    "    labels.append(img_batch[\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ac87bfc-6f03-4047-9b07-ac89eb54a0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.29      0.33       708\n",
      "           1       0.36      0.63      0.46       680\n",
      "           2       0.40      0.02      0.04       208\n",
      "           3       0.40      0.38      0.39       759\n",
      "           4       0.27      0.52      0.36       454\n",
      "           5       0.44      0.22      0.30       516\n",
      "           6       0.47      0.22      0.30       481\n",
      "\n",
      "    accuracy                           0.36      3806\n",
      "   macro avg       0.39      0.33      0.31      3806\n",
      "weighted avg       0.39      0.36      0.34      3806\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dd7a7fda-653f-4763-99dc-487dcf294d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10869076017138635\n"
     ]
    }
   ],
   "source": [
    "from imblearn.metrics import specificity_score\n",
    "print(1 - specificity_score(labels, preds, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b57c3ec-e39a-4577-9477-f1b1707c0535",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
