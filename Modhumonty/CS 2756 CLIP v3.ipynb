{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8e3d6c8-452c-45c9-9261-896fd975c3fb",
   "metadata": {},
   "source": [
    "## Test Split Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cea36330-6e92-4a60-876c-a3a023feee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6840ab7d-9ceb-49e7-aa1c-b091e01949d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_directory = 'E:/Pitt/Spring 2024/CS 2002/wikiart/wikiart'\n",
    "new_folder_path = 'E:/Pitt/Spring 2024/CS 2002/wikiart/CS 2756/v3_Test' \n",
    "os.makedirs(new_folder_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57b217b3-06c7-4631-880b-e9f49d3a80cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df =  pd.read_csv('E:/Pitt/Spring 2024/CS 2002/final-splits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dcd1e83-e1d4-4498-91d3-906fe4e90774",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = test_df[test_df['split'] == 'TEST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "899ed2dc-5c97-420a-a116-734f7700ca35",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "folder = []\n",
    "painting = []\n",
    "def copy_sampled_images(sampled_df, parent_directory, new_folder_path):\n",
    "    file_extensions = ['.jpg', '.jpeg', '.png']  \n",
    "\n",
    "    for _, row in sampled_df.iterrows():\n",
    "        painting_name = row['painting']\n",
    "        folder_name = row['art_style']\n",
    "        folder.append(folder_name)\n",
    "        painting.append(painting_name)\n",
    "          \n",
    "        # Path to the sub-folder\n",
    "        sub_folder_path = os.path.join(parent_directory, folder_name)\n",
    "        \n",
    "        found = False\n",
    "        for ext in file_extensions:\n",
    "            painting_path = os.path.join(sub_folder_path, painting_name + ext)\n",
    "            if os.path.exists(painting_path):\n",
    "                shutil.copy(painting_path, new_folder_path)\n",
    "                #print(f\"Copied: {painting_path}\")\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            print(f\"Painting not found: {painting_name} in {sub_folder_path}\")\n",
    "\n",
    "copy_sampled_images(sampled_df, parent_directory, new_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ed164e8-08a0-42e9-9378-0e68cb0b089e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3800"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a29f516d-6382-4543-99c4-0983d4d14b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3800"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sampled_df['painting'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dccaec99-4798-433f-a225-d61923cc92eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "033a02a7-d679-487e-bb4a-1f8e2ee66391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.2.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pkg_resources import packaging\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e8dc92e-a02d-480c-bb6e-3affa6943914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import clip\n",
    "\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3711df6-70f4-47f8-a10f-462f9c146379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 151,277,313\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "model.cuda().eval()\n",
    "# model.cpu().eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ac35806-83a2-406e-8678-254efd19127a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    <function _convert_image_to_rgb at 0x0000024BD5DFE430>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10535ef-187c-4ca3-b220-fcf1766df819",
   "metadata": {},
   "source": [
    "## Setting up input images and texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05c80cb3-fadc-4407-afda-f7ab09156925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "\n",
    "images_dir = 'E:/Pitt/Spring 2024/CS 2002/wikiart/CS 2756/v3_Test' \n",
    "#descriptions_csv = pd.read_csv('E:/Pitt/Spring 2024/CS 2002/final-splits.csv')\n",
    "descriptions_df = pd.read_csv('E:/Pitt/Spring 2024/CS 2002/final-splits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcadad50-cc43-457e-9755-123db0bced97",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTEMIS_EMOTIONS = ['excitement-amusement',\n",
    " 'awe',\n",
    " 'contentment',\n",
    " 'disgust-anger',\n",
    " 'fear',\n",
    " 'sadness',\n",
    " 'something else']\n",
    "\n",
    "emotions = [\"A picture making me feel \" + emotion for emotion in ARTEMIS_EMOTIONS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a716409-aa27-4b91-bf38-da904aa3c2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3800\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_images = sum(1 for file in os.listdir(images_dir) if file.endswith(('.png', '.jpg', '.jpeg')))\n",
    "print(total_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24eb4af4-7cd5-43ba-ac46-05fc58cfb0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "image_filenames = [filename for filename in os.listdir(images_dir) if filename.endswith(\".png\") or filename.endswith(\".jpg\")]\n",
    "image_filenames_ext = [[os.path.splitext(filename)[0] for filename in os.listdir(images_dir) if filename.endswith(\".png\") or filename.endswith(\".jpg\")]]\n",
    "\n",
    "def process_in_batches(image_filenames, images_dir, text_tokens, model, preprocess, batch_size=32):\n",
    "    all_top_probs = []\n",
    "    all_top_labels = []\n",
    "\n",
    "    # total number of batches\n",
    "    total_batches = len(image_filenames) // batch_size + (1 if len(image_filenames) % batch_size > 0 else 0)\n",
    "\n",
    "    for batch_idx in range(total_batches):\n",
    "        # start and end indices for the current batch\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        batch_filenames = image_filenames[start_idx:end_idx]\n",
    "\n",
    "        # preprocess images in the current batch\n",
    "        processed_images = [preprocess(Image.open(os.path.join(images_dir, filename)).convert(\"RGB\")) for filename in batch_filenames]\n",
    "        image_input = torch.tensor(np.stack(processed_images)).cuda()\n",
    "\n",
    "        with torch.no_grad():  # Encoding\n",
    "            image_features = model.encode_image(image_input).float()\n",
    "            # Normalize features\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # Calculate probabilities for the current batch\n",
    "            text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "            top_probs, top_labels = text_probs.cpu().topk(5, dim=-1)\n",
    "\n",
    "        # Accumulate results from the current batch\n",
    "        all_top_probs.append(top_probs)\n",
    "        all_top_labels.append(top_labels)\n",
    "\n",
    "    # Concatenate results from all batches\n",
    "    all_top_probs = torch.cat(all_top_probs, dim=0)\n",
    "    all_top_labels = torch.cat(all_top_labels, dim=0)\n",
    "\n",
    "    return all_top_probs, all_top_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c41d067-9e37-49b3-ab0d-e2a958da9926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Downloads\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "text_tokens = clip.tokenize(emotions).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_tokens).float()\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f74d329-63e2-4532-9acd-2916ee272e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 \n",
    "all_top_probs, all_top_labels = process_in_batches(\n",
    "    image_filenames, images_dir, text_tokens, model, preprocess, batch_size=batch_size\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cafac4a-4b18-40cd-9e2e-d33fe72c7b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_top_probs[10])\n",
    "print(all_top_labels[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a8a21-1abb-4e80-af79-0ad92057abfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filenames_ext = [os.path.splitext(filename)[0] for filename in os.listdir(images_dir) if filename.endswith(\".png\") or filename.endswith(\".jpg\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00b207a-5384-485f-94b3-3666aaeddcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_label = np.zeros(len(image_filenames_ext),dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b264e267-e2fe-4b86-b68a-9829dc1eb76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, painting in enumerate(image_filenames_ext):\n",
    "    \n",
    "    painting_df = descriptions_df[descriptions_df['painting'] == painting]\n",
    "    if i == 0:\n",
    "      print(painting)\n",
    "      print(painting_df)\n",
    "    for _, row in painting_df.iterrows():\n",
    "        emotion_index = ARTEMIS_EMOTIONS.index(row['emotion'])\n",
    "        gt_label[i] = emotion_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c376dd-1ffa-42e3-b774-a997a3b2bb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTEMIS_EMOTIONS[gt_label[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e309d543-82dd-4d94-a8e5-7d844fa36a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_probs_cpu = all_top_probs.cpu().numpy()\n",
    "print(top_probs_cpu[0])\n",
    "print(np.argmax(top_probs_cpu[0]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954dce41-eea2-4f40-aeb6-a75b1ce3b07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "correct_predictions = 0\n",
    "\n",
    "\n",
    "for i in range(len(gt_label)):\n",
    "    # If the ground truth label is among the top 5 predictions\n",
    "    if gt_label[i] in all_top_labels[i]:\n",
    "        correct_predictions += 1\n",
    "\n",
    "\n",
    "accuracy = correct_predictions / len(gt_label)\n",
    "\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7277ba74-a30a-42fe-b84d-35572702f863",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "correct_predictions = 0\n",
    "\n",
    "for i in range(len(gt_label)):\n",
    "    # If the ground truth label is equal to the top prediction\n",
    "    if gt_label[i] == all_top_labels[i][0]:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / len(gt_label)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbea51f-1aa1-46df-8211-79dd7528be1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "max_pred = all_top_labels[:, 0]\n",
    "\n",
    "cm = confusion_matrix(gt_label, max_pred, labels=np.arange(len(ARTEMIS_EMOTIONS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da98723-b3f4-4f5f-b667-e8933f251531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=ARTEMIS_EMOTIONS, yticklabels=ARTEMIS_EMOTIONS)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464aa63d-684c-419d-9910-dd044d910de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "cm_normalized = np.nan_to_num(cm_normalized)  # Replace NaNs with 0\n",
    "\n",
    "fig = ff.create_annotated_heatmap(z=cm_normalized, x=ARTEMIS_EMOTIONS, y=ARTEMIS_EMOTIONS, colorscale='Blues', annotation_text=np.around(cm_normalized, decimals=2))\n",
    "fig.update_layout(title=f'Confusion Matrix - Normalized',\n",
    "                  xaxis=dict(title='Predicted'),\n",
    "                  yaxis=dict(title='Actual'))\n",
    "\n",
    "fig.update_layout(margin=dict(t=50, l=200))\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38891f5d-4515-4756-90ba-8dba0a4c5430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eeb175-1c79-493c-b8fb-bc96944dd18e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
