{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a729664-ce22-4bd6-a149-d6235a4a89dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install ftfy regex tqdm\n",
    "! pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d814fa2-7932-434e-8f1a-f36234c112de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81e54b3a-61b7-401f-bfc4-7c3a7c72477b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.2.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pkg_resources import packaging\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ad95b49-3d5e-4c3c-89e7-019f05b37813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import clip\n",
    "\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa24ec6d-5c95-4114-8710-22950d835cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 151,277,313\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "model.cuda().eval()\n",
    "# model.cpu().eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3882ed38-6599-4017-ba98-13698a980067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    <function _convert_image_to_rgb at 0x000001A838D76040>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f733742-2cab-49cd-8231-c07ada05b831",
   "metadata": {},
   "source": [
    "## Setting up input images and texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a93f98f7-ceb4-4630-a705-3a763e3aaab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image,ImageFile\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "images_dir = 'E:/Pitt/Spring 2024/CS 2002/wikiart/random_samples'\n",
    "descriptions_csv = 'E:/Pitt/Spring 2024/CS 2002/artemis_official_data/official_data/artemis_dataset_release_v0.csv'\n",
    "\n",
    "painting_style = os.path.basename(images_dir)\n",
    "\n",
    "# Load the textual descriptions from a CSV file into a dictionary\n",
    "descriptions_df = pd.read_csv(descriptions_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0220cb7-e443-4de5-9491-271923b2ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTEMIS_EMOTIONS = ['amusement',\n",
    " 'awe',\n",
    " 'contentment',\n",
    " 'excitement',\n",
    " 'anger',\n",
    " 'disgust',\n",
    " 'fear',\n",
    " 'sadness',\n",
    " 'something else']\n",
    "\n",
    "emotions = [\"A picture making me feel \" + emotion for emotion in ARTEMIS_EMOTIONS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc54b150-0cc0-4eb7-b2bd-ebac93cb71c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5235\n"
     ]
    }
   ],
   "source": [
    "# Check total number of images in the directory\n",
    "total_images = sum(1 for file in os.listdir(images_dir) if file.endswith(('.png', '.jpg', '.jpeg')))\n",
    "print(total_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a91e2d4-8680-4f2b-8070-35d2b45cc4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "#original_images = []\n",
    "processed_images = []\n",
    "#plt.figure(figsize=(16,5))\n",
    "\n",
    "# Retrieve the filenames and filter out non-image files and images without descriptions\n",
    "image_filenames = [filename for filename in os.listdir(images_dir) if filename.endswith(\".png\") or filename.endswith(\".jpg\")]\n",
    "\n",
    "#image_filenames = [filename for filename in image_filenames if os.path.splitext(filename)[0] in descriptions]\n",
    "image_filenames_ext = []\n",
    "# Allow PIL to load truncated images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "for i, filename in enumerate(image_filenames):\n",
    "    name = os.path.splitext(filename)[0]\n",
    "    image_filenames_ext.append(name)\n",
    "\n",
    "    try:\n",
    "        image = Image.open(os.path.join(images_dir, filename)).convert(\"RGB\")\n",
    "        image_filenames_ext.append(name)\n",
    "        #original_images.append(image)\n",
    "        processed_images.append(preprocess(image))\n",
    "    except IOError as e:\n",
    "            print(\"Unable to load image:\", e)\n",
    "\n",
    "    #original_images.append(image)\n",
    "\n",
    "\n",
    "#print(len(processed_images))\n",
    "del image_filenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c9561c1-f4f1-484e-a6ac-759e45f08925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5235\n",
      "454684\n"
     ]
    }
   ],
   "source": [
    "print(len(processed_images))\n",
    "#print(len(texts))\n",
    "print(len(descriptions_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c37eae-e2e2-429f-aa8d-d39278b65ddc",
   "metadata": {},
   "source": [
    "## Building Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b79d9a4b-3c5f-4173-8772-d22301002d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5235, 3, 224, 224])\n",
      "torch.Size([9, 77])\n"
     ]
    }
   ],
   "source": [
    "image_input = torch.tensor(np.stack(processed_images)).cuda()\n",
    "text_tokens = clip.tokenize(emotions).cuda()\n",
    "\n",
    "print (image_input.shape)\n",
    "print (text_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db7acb2c-e334-4b04-ab62-ba16887f1f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Memory Error\n",
    "del processed_images\n",
    "gc.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2211dd17-6312-4c67-8722-95777a3abcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(): #Encoding\n",
    "    image_features = model.encode_image(image_input).float()\n",
    "    text_features = model.encode_text(text_tokens).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e2f6c8-58ce-4746-b715-f0b07059e486",
   "metadata": {},
   "outputs": [],
   "source": [
    "del image_input\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c0ee4-f808-45e4-b345-310009b2dfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize features\n",
    "\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2ffdcf-7e0b-438d-a521-fb2466ad8fab",
   "metadata": {},
   "source": [
    "## ZERO SHOT Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f90946-b1e0-4f90-841c-aaaf668d37d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "top_probs, top_labels = text_probs.cpu().topk(5, dim=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
